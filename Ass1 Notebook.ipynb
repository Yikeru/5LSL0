{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb619cfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2230691989.py, line 72)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [5]\u001b[1;36m\u001b[0m\n\u001b[1;33m    data_loc = 'C:\\Users\\20167271\\Desktop\\ML for signal processing\\A1\\MNIST' #change the datalocation to something that works for you\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# %% imports\n",
    "# pytorch\n",
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "# pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %% Noisy MNIST dataset\n",
    "class Noisy_MNIST(Dataset):\n",
    "    # initialization of the dataset\n",
    "    def __init__(self, split,data_loc,noise=0.5):\n",
    "        # save the input parameters\n",
    "        self.split    = split \n",
    "        self.data_loc = data_loc\n",
    "        self.noise    = noise\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            Train = True\n",
    "        else:\n",
    "            Train = False\n",
    "            \n",
    "        # get the original MNIST dataset   \n",
    "        Clean_MNIST = datasets.MNIST(self.data_loc, train=Train, download=True)\n",
    "        \n",
    "        # reshuffle the test set to have digits 0-9 at the start\n",
    "        if self.split == 'train':\n",
    "            data = Clean_MNIST.data.unsqueeze(1)\n",
    "        else:\n",
    "            data = Clean_MNIST.data.unsqueeze(1)\n",
    "            idx = torch.load('test_idx.tar')\n",
    "            data[:,:] = data[idx,:]\n",
    "            \n",
    "        \n",
    "        # reshape and normalize\n",
    "        resizer = transforms.Resize(32)\n",
    "        resized_data = resizer(data)*1.0\n",
    "        normalized_data = 2 *(resized_data/255) - 1\n",
    "        #normalized_data = (resized_data - 33)/74\n",
    "        \n",
    "        # create the data\n",
    "        self.Clean_Images = normalized_data\n",
    "        self.Noisy_Images = normalized_data + torch.randn(normalized_data.size())*self.noise\n",
    "        self.Labels       = Clean_MNIST.targets\n",
    "    \n",
    "    # return the number of examples in this dataset\n",
    "    def __len__(self):\n",
    "        return self.Labels.size(0)\n",
    "    \n",
    "    # create a a method that retrieves a single item form the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        clean_image = self.Clean_Images[idx,:,:,:]\n",
    "        noisy_image = self.Noisy_Images[idx,:,:,:]\n",
    "        label =  self.Labels[idx]\n",
    "        \n",
    "        return clean_image,noisy_image,label\n",
    "    \n",
    "# %% dataloader for the Noisy MNIST dataset\n",
    "def create_dataloaders(data_loc, batch_size):\n",
    "    Noisy_MNIST_train = Noisy_MNIST(\"train\", data_loc)\n",
    "    Noisy_MNIST_test  = Noisy_MNIST(\"test\" , data_loc)\n",
    "    \n",
    "    Noisy_MNIST_train_loader =  DataLoader(Noisy_MNIST_train, batch_size=batch_size, shuffle=True,  drop_last=False)\n",
    "    Noisy_MNIST_test_loader  =  DataLoader(Noisy_MNIST_test , batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    return Noisy_MNIST_train_loader, Noisy_MNIST_test_loader\n",
    "\n",
    "# %% test if the dataloaders work\n",
    "if __name__ == \"__main__\":\n",
    "    # define parameters\n",
    "    data_loc = r'C:\\Users\\20167271\\Desktop\\ML for signal processing\\A1\\MNIST' #change the datalocation to something that works for you\n",
    "    batch_size = 64\n",
    "    \n",
    "    # get dataloader\n",
    "    train_loader, test_loader = create_dataloaders(data_loc, batch_size)\n",
    "    \n",
    "    # get some examples\n",
    "    examples = enumerate(test_loader)\n",
    "    _, (x_clean_example, x_noisy_example, labels_example) = next(examples)\n",
    "    # use these example images througout the assignment as the first 10 correspond to the digits 0-9\n",
    "    \n",
    "    # show the examples in a plot\n",
    "    plt.figure(figsize=(12,3))\n",
    "    for i in range(10):\n",
    "        plt.subplot(2,10,i+1)\n",
    "        plt.imshow(x_clean_example[i,0,:,:],cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        \n",
    "        plt.subplot(2,10,i+11)\n",
    "        plt.imshow(x_noisy_example[i,0,:,:],cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"data_examples.png\",dpi=300,bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61ab7697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exc 1b)\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83834349",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "dir_data = os.path.abspath(\"data\")\n",
    "dir_truth = os.path.join(dir_data, \"gtFine\")\n",
    "torch.save(model.state_dict(), os.path.join(\"./\", \"model_unet_base_line.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f37df020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_1b(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Feed Forward Neural Network\n",
    "    \n",
    "    Param\n",
    "    ------------\n",
    "    input_size : int (default = 786)         \n",
    "        The length of the datasets which is width*height)\n",
    "    n_hidden : int (default: 200)\n",
    "        Number of hidden units.\n",
    "    n_classes: int  (default = 10)        \n",
    "        Number of the final classes, in our case 0-9 thus 10 classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=786, hidden_size= 200, n_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flat = nn.Flatten(),\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size),\n",
    "\n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, n_classes)\n",
    "        #no activation functions yet\n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        y = self.network(x)\n",
    "        return y\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \"\"\"Predict class labels\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # Implement prediction here\n",
    "        a_out = self.forward(x).detach().numpy()\n",
    "        result = np.zeros(len(a_out))\n",
    "        for i in range(len(a_out)):\n",
    "            j = np.argmax(a_out[i])\n",
    "            result[i] = j\n",
    "        y_pred = result\n",
    "        return y_pred\n",
    "    \n",
    "model = FFNN_1b()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a82ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part34(model, optimizer, epochs=1, write_to_file=False, USE_GPU=True):\n",
    "    \"\"\"\n",
    "    Fit the model on the training data set.\n",
    "    Arguments\n",
    "    ---------\n",
    "    model : model class\n",
    "        Model structure to fit, as defined by build_model().\n",
    "    epochs :  int\n",
    "        Number of epochs the training should do\n",
    "    optimizer : optim class\n",
    "        Optimizer to use\n",
    "    write_to_file : bool (default:False)\n",
    "        Write model to file; can later be loaded through load_model()\n",
    "    USE_GPU : bool (default:True\n",
    "        Parameters that when set to True uses the GPU for the training\n",
    "    Returns\n",
    "    -------\n",
    "    model : model class\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "    # First check if we have GPU we can use to train\n",
    "    dtype = torch.float32\n",
    "    if USE_GPU and torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    model = model.to(device=device)  # Move the model parameters to CPU/GPU\n",
    "    print_every = 100 # Constant to control how frequently we print train loss\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()\n",
    "\n",
    "    #### Save the model\n",
    "    print(acc)\n",
    "    if write_to_file:        \n",
    "        # Save the weights of the model to a .pt file\n",
    "#         print(MODEL_NEW)\n",
    "#         torch.save(model.state_dict(), MODEL_NEW)\n",
    "        torch.save(model.state_dict(), os.path.join(\"./\", \"Saved_Model.pth\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "138c466c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Noisy_MNIST_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(\u001b[43mNoisy_MNIST_test\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Noisy_MNIST_test' is not defined"
     ]
    }
   ],
   "source": [
    "pred = model(Noisy_MNIST_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b2b5a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_accuracy_part2(loader, model_fn, params):\n",
    "#     \"\"\"\n",
    "#     Check the accuracy of a classification model.\n",
    "    \n",
    "#     Inputs:\n",
    "#     - loader: A DataLoader for the data split we want to check\n",
    "#     - model_fn: A function that performs the forward pass of the model,\n",
    "#       with the signature scores = model_fn(x, params)\n",
    "#     - params: List of PyTorch Tensors giving parameters of the model\n",
    "    \n",
    "#     Returns: Nothing, but prints the accuracy of the model\n",
    "#     \"\"\"\n",
    "#     split = 'val' if loader.dataset.train else 'test'\n",
    "#     print('Checking accuracy on the %s set' % split)\n",
    "#     num_correct, num_samples = 0, 0\n",
    "#     with torch.no_grad():\n",
    "#         for x, y in loader:\n",
    "#             x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "#             y = y.to(device=device, dtype=torch.int64)\n",
    "#             scores = model_fn(x, params)\n",
    "#             _, preds = scores.max(1)\n",
    "#             num_correct += (preds == y).sum()\n",
    "#             num_samples += preds.size(0)\n",
    "#         acc = float(num_correct) / num_samples\n",
    "#         print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6582fcdad78c9817c7b8382b86eb25db2029e48a96f1080b267c5349ec26f2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
