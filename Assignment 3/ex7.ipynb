{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20167271\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %% imports\n",
    "# libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.activation import ReLU\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.ticker as plticker\n",
    "import os\n",
    "\n",
    "# local imports\n",
    "import MNIST_dataloader\n",
    "import AEarchitecture\n",
    "from config_file import data_loc, batch_size\n",
    "from MNIST_dataloader import *\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()  # same as super().__init__()\n",
    "        # how to find output shape: [input - kernel + 2*padding - (dilation-1)(kernel-1)]/stride + 1\n",
    "        self.encoder = nn.Sequential(\n",
    "            #  \n",
    "            nn.Conv2d(1,16, kernel_size=(3,3),padding=1, stride=1), #[Nx1x32x32]=> [Nx16x32x32]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),#[Nx16x32x32]=> [Nx16x16x16]\n",
    "\n",
    "            nn.Conv2d(16,16, kernel_size=(3,3),padding=1, stride=1), #[Nx16x16x16]=> [Nx16x16x16]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),#[Nx16x16x16]=> [Nx16x8x8]\n",
    "\n",
    "            nn.Conv2d(16,16, kernel_size=(3,3),padding=1, stride=1), #[Nx16x8x8]=> [Nx16x8x8]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),#[Nx16x8x8]=> [Nx16x4x4]\n",
    "\n",
    "            nn.Conv2d(16,16, kernel_size=(3,3),padding=1, stride=1), #[Nx16x4x4]=> [Nx16x4x4]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),#[Nx16x4x4]=> [Nx16x2x2]\n",
    "\n",
    "            nn.Conv2d(16,2, kernel_size=(3,3),padding=1, stride=1), #[Nx16x2x2]=> [Nx1x2x2]\n",
    "            nn.MaxPool2d((1,2)) #[Nx1x2x2]=> [Nx1x2x1]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # First branch uses dilation\n",
    "        y = self.encoder(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2, 16, kernel_size=(3,3)), # [Nx1x2x1]=>[Nx16x4x3]\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2), #[Nx16x4x3]=>[Nx16x8x6]\n",
    "\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=(3,3)), # [Nx16x8x6]=>[Nx16x10x8]\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2), #[Nx16x10x8]=>[Nx16x20x16]\n",
    "\n",
    "            nn.ConvTranspose2d(16, 16, kernel_size=(3,3)), # [Nx16x10x8]=>[Nx16x22x18]\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2), #[Nx16x22x18]=>[Nx16x44x36]\n",
    "\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=(3,3)), # [Nx16x44x36]=>[Nx1x46x38]\n",
    "            nn.Upsample(size=(32,32)), #[Nx1x46x38]=>[Nx1x32x32]\n",
    "            )\n",
    "\n",
    "    def forward(self, h):\n",
    "        # use the created layers here\n",
    "        return self.decoder(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%  Autoencoder\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(\"Setting up the autoencoder\")\n",
    "#         print(\"We re feeding the autoencoder input of shape \", x.size())\n",
    "        h = self.encoder(x)\n",
    "        r = self.decoder(h)\n",
    "        return r, h\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create fucntion to sample from latent space\n",
    "\n",
    "def sample(xmean: torch.Tensor, logstd: torch.Tensor) -> torch.Tensor:\n",
    "    '''outputs a vector of the same length where v[k] = xmean[k] + n*std[k] \n",
    "    where n is sampled from the normal distribution'''\n",
    "\n",
    "    assert xmean.size(0) == logstd.size(0), \"Vectors of emans and standard deviations have different lengths\" \n",
    "    l = xmean.size(0)\n",
    "    n = np.random.rand(n)\n",
    "\n",
    "    return xmean + n * torch.exp(logstd)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6582fcdad78c9817c7b8382b86eb25db2029e48a96f1080b267c5349ec26f2c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
