{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises Week 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20167271\\Desktop\\ML for signal processing\\5LSL0-GIT\\Assignment 4\n"
     ]
    }
   ],
   "source": [
    "# %% imports\n",
    "# libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# local imports\n",
    "import Fast_MRI_dataloader\n",
    "# import autoencoder_template\n",
    "from matplotlib import pyplot as plt\n",
    "from Fast_MRI_dataloader import create_dataloaders\n",
    "\n",
    "# set-up folders\n",
    "data_loc = os.getcwd()\n",
    "print(data_loc)\n",
    "data_loc = data_loc + '\\Fast_MRI_Knee'\n",
    "batch_size = 16\n",
    "train_loader, test_loader = create_dataloaders(data_loc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_img(X):\n",
    "    '''\n",
    "    Given the partial k_space image X, calculate the accelerated measurement image:\n",
    "\n",
    "    Arguments:\n",
    "    ----------------\n",
    "    X: torch.tensor()\n",
    "\n",
    "    returns:\n",
    "    --------------\n",
    "    Y: torch.tensor()\n",
    "    '''\n",
    "    Y = torch.fft.ifft2(torch.fft.ifftshift(X, dim=(-2,-1)))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_space(gt):\n",
    "    ''' Return the K-space of an input image:\n",
    "        Arguments:\n",
    "        ---------------------\n",
    "        X: torch.Tensor (default: -)\n",
    "\n",
    "        Returns:\n",
    "        ---------------------------\n",
    "        KS: torch.Tensor\n",
    "    '''\n",
    "    # KS = torch.log(torch.abs(torch.fft.fftshift(torch.fft.fft2(gt)))) \n",
    "    KS = torch.fft.fftshift(torch.fft.fft2(gt))\n",
    "    return KS\n",
    "\n",
    "impulse=torch.zeros((320,320))\n",
    "impulse[160:162,160:162] = 1\n",
    "A = torch.abs(K_space(impulse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.abs(A).numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - ConvNet\n",
    "As a starting point for a deep learning solution, design an end-to-end convolutional neural\n",
    "network. As input it should take an initial reconstruction from a partial k-space measurement\n",
    "and output a guess for the final reconstruction. The net is not (yet) allowed to make\n",
    "use of knowledge about the sampling mask.\n",
    "\n",
    "(a) [1pt] What loss function would be most suitable to train this network, and what\n",
    "underlying assumption have you made by choosing this loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence is a metric to compare the distribution of pixel values between the input and output images. The MSE loss can be suitable as well but we already know that there is loss of spatial information given the mask and we do not want to penalize shifts or blurring, we want the network to reconstruct the original scan as similarly as possible with the input. Since the network does not know the mask, it should penalize the difference in distributions rather than shift in image. We assume the input distribution is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [3pt] Design a convolutional neural network of no more than 4 layers. Train it for at\n",
    "least 10 epochs and plot both the training loss and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### BAD MODEL ################################\n",
    "class KReconstruct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KReconstruct, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            #nn.Conv2d(2, 4, kernel_size=12, stride=2, padding=0,dilation=1),  # 155x155\n",
    "            #nn.BatchNorm2d(4),\n",
    "            #nn.ReLU(),\n",
    "\n",
    "            #nn.Conv2d(4, 16, kernel_size=7, stride=1, padding=0, dilation=2), # 143x143\n",
    "            #nn.BatchNorm2d(16),\n",
    "            #nn.ReLU(),\n",
    "\n",
    "            #nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0, dilation=1), # 139x139\n",
    "            #nn.BatchNorm2d(32),\n",
    "            #nn.ReLU(),\n",
    "\n",
    "            #nn.ConvTranspose2d(32, 1, kernel_size=23, stride=2, padding=1, dilation=2, output_padding=1) # 320x320\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #nn.Conv2d(2,4, kernel_size=7, stride=1, dilation=2),  \n",
    "            #nn.BatchNorm2d(4),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.Conv2d(4,8, kernel_size=5, stride=1, dilation=2),  \n",
    "            #nn.BatchNorm2d(8),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.Conv2d(8,16, kernel_size=3, stride=1, dilation=1),  \n",
    "            #nn.BatchNorm2d(16),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.ConvTranspose2d(16, 1, kernel_size=13, stride=1, padding=1, dilation=2, output_padding=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #nn.Conv2d(1,4, kernel_size=5, stride=1, padding=4, dilation=2),\n",
    "            #nn.BatchNorm2d(4),\n",
    "            #nn.ReLU(),\n",
    "            \n",
    "            #nn.Conv2d(4,8, kernel_size=5, stride=1, padding=2, dilation=1),\n",
    "            \n",
    "            #nn.Conv2d(8,4, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "            \n",
    "            #nn.Conv2d(4,1, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(1,8, kernel_size=5, stride=1, padding=2, dilation=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv2d(8,16, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ELU(),\n",
    "            \n",
    "            nn.Conv2d(16,1, kernel_size=3, stride=1, padding=1, dilation=1),\n",
    "            nn.ReLU()\n",
    "            \n",
    "            #nn.Conv2d(4,1, kernel_size=3, stride=1, padding=1, dilation=1)\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        xabs = torch.abs(x)\n",
    "        \n",
    "        img = self.model(xabs)\n",
    "        #out = nn.functional.log_softmax(img)\n",
    "        \n",
    "        return img#, out\n",
    "    \n",
    "    def test(self, x):\n",
    "        #xreal = torch.real(x)\n",
    "        #ximag = torch.imag(x)\n",
    "        #x = torch.cat((xreal, ximag), 1)\n",
    "        xabs = torch.abs(x)\n",
    "        \n",
    "        return self.model(xabs)\n",
    "\n",
    "\n",
    "model = KReconstruct()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5e-3, weight_decay=0.0005)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kspace, M, gt = next(iter(test_loader))\n",
    "m = KReconstruct()\n",
    "this = m.test(acc_img(kspace).unsqueeze(1))\n",
    "this.squeeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin training for 10 epochs\n",
    "\n",
    "epochs=10\n",
    "# set device\n",
    "dtype = torch.float32\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Running on: ', device)\n",
    "model = model.to(device=device)\n",
    "# keep track of losses\n",
    "eval_dic = {'Loss_t': [], 'train_acc': [],'Loss_v': [], 'valid_acc': []}\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "loss_dict = {'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "\n",
    "print(f\"\\nTraining is about to start on {device}\\n\")\n",
    "for e in range(epochs):\n",
    "    print(f\"\\nTraining epoch {e}: \\t\")\n",
    "    mia_loss = 0\n",
    "    for batch_idx,(kspace, M, gt) in enumerate(tqdm(train_loader)):\n",
    "        # prepare for training\n",
    "        model.train(True)  \n",
    "        optimizer.zero_grad()\n",
    "        # move to gpu if available\n",
    "        reconstructed_img = acc_img(kspace).to(device=device)\n",
    "        #M = M.to(device=device)\n",
    "        gt = gt.to(device=device)\n",
    "        # forward pass\n",
    "        out = model(reconstructed_img.unsqueeze(1))\n",
    "        out = out.squeeze(1)\n",
    "        # backpropagate loss\n",
    "        loss = criterion(out, gt)\n",
    "        loss = loss.to(torch.float32)\n",
    "        loss.backward()\n",
    "        mia_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # update loss dictionary\n",
    "    loss_dict['train_loss'].append(mia_loss/(batch_idx+1))\n",
    "\n",
    "    # print epoch loss\n",
    "    print(f\"\\nEpoch training loss is {mia_loss/(batch_idx+1)}\")\n",
    "\n",
    "    with torch.no_grad():  # setting so that no backprop happens during validation\n",
    "        model.eval()  # setting so that no backprop happens during validation\n",
    "        vloss = 0\n",
    "        for (kspace, M, gt) in tqdm(test_loader):\n",
    "            reconstructed_img = acc_img(kspace).to(device=device)\n",
    "            #M = M.to(device=device)\n",
    "            gt = gt.to(device=device)\n",
    "            out = model(reconstructed_img.unsqueeze(1))\n",
    "            out = out.squeeze(1)\n",
    "            vloss += criterion(out, gt).to(torch.float16).item()\n",
    "        # update loss dictionary  \n",
    "        loss_dict['val_loss'].append(vloss/len(test_loader))\n",
    "        # print validation loss\n",
    "        print(f\"\\nEpoch validation loss is {vloss/len(test_loader)}\")\n",
    "\n",
    "# save trained model\n",
    "model_path = os.getcwd() + '\\Modelex5ELU.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "\n",
    "#validation loss is actually test loss\n",
    "plt.figure(figsize=(15,10),dpi=100)\n",
    "plt.plot(range(epochs), loss_dict['train_loss'], \n",
    "         label='training loss', linewidth=5)\n",
    "plt.plot(range(epochs), np.array(loss_dict['val_loss']), \n",
    "         label='test loss', linestyle='--', linewidth=5)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.grid()\n",
    "plt.legend(loc=0, prop={'size': 20})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [2pt] Repeat exercise 4c using your trained ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_path = os.getcwd() + '\\Modelex5ELU.pth'\n",
    "model = KReconstruct()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "model.to('cpu')\n",
    "\n",
    "kspace,Mask,gt = next(iter(test_loader))\n",
    "# (mu,shrinkage,K,kspace,mask)\n",
    "plt.figure(figsize=(15,20))\n",
    "for i in range(5):\n",
    "\n",
    "    rec_init_kp =acc_img(kspace[i:i+1,:,:]) # init recon from k-space\n",
    "    output = model.forward(rec_init_kp.unsqueeze(1)) # recon mri_ista\n",
    "    gt_MRI = gt[i,:,:]\n",
    "\n",
    "    \n",
    "\n",
    "    plt.subplot(3,5,i+1)\n",
    "    plt.imshow(torch.abs(rec_init_kp.squeeze(0)),cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Init reconstruction\")\n",
    "    \n",
    "    plt.subplot(3,5,i+6)\n",
    "    plt.imshow(output.squeeze(0).squeeze(0).detach(),cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Model output\")\n",
    "    \n",
    "    plt.subplot(3,5,i+11)\n",
    "    plt.imshow(torch.abs(gt_MRI),cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"Ground Truth\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '\\Modelex5ELU.pth'\n",
    "model = KReconstruct()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "MSEtest=0\n",
    "for (kspace,Mask,gt) in test_loader:\n",
    "    reconstruct = acc_img(kspace)\n",
    "    filteredwithCNN  = model.forward(reconstruct.unsqueeze(1)).squeeze(1)\n",
    "    MSEtest += nn.functional.mse_loss(gt,filteredwithCNN).item()\n",
    "print(f\"Mean of the mse ON THE test set is: {MSEtest/len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "We shall now combine the strenghts of ISTA with that of our deep learning ConvNet to\n",
    "arrive ate some state-of-the-art results in reconstruction of accelerated MRI. To that end,\n",
    "we shall be employing neural proximal gradient descent. This method is simmilar to LISTA,\n",
    "but with some slight changes.\n",
    "\n",
    "(b) [4pt] Implement neural proximal gradient descent for 5 unfolded iterations. As proximal\n",
    "operator, make use of the ConvNet you designed in exercise 5. Train it for at\n",
    "least 10 epochs and plot both the training loss and testing loss in terms of epochs or\n",
    "batches seen.\n",
    "hint: μ should remain between 0 and 1. Moreover, it helps to set a higher learning rate\n",
    "for the μs compared to the other trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.getcwd() + '\\Modelex5ELU.pth'\n",
    "Proxi = KReconstruct()\n",
    "Proxi.load_state_dict(torch.load(model_path))\n",
    "Proxi.train(True)\n",
    "\n",
    "class newISTA(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(newISTA,self).__init__()   \n",
    "        self.device=device\n",
    "        self.mu1 = nn.parameter.Parameter(torch.ones(1)).to(device=self.device)\n",
    "        #self.mu3 = nn.parameter.Parameter(torch.ones(1)).to(device=self.device)\n",
    "        self.mu3 = torch.tensor(0.8, requires_grad=True)\n",
    "        self.mu5 = nn.parameter.Parameter(torch.ones(1)).to(device=self.device)\n",
    "        self.mu7 = nn.parameter.Parameter(torch.ones(1)).to(device=self.device)\n",
    "        #self.mu9 = nn.parameter.Parameter(torch.ones(1)).to(device=self.device)\n",
    "        #self.mu11 = nn.parameter.Parameter(torch.ones(1)).to(device=self.device)\n",
    "\n",
    "        self.proxi1 = Proxi.to(device=self.device).train()\n",
    "\n",
    "        self.proxi3 = KReconstruct().to(device=self.device).train()\n",
    "\n",
    "        self.proxi5 = KReconstruct().to(device=self.device).train()\n",
    "        \n",
    "        self.proxi7 = KReconstruct().to(device=self.device).train()\n",
    "        \n",
    "        self.proxi9 = KReconstruct().to(device=self.device).train()\n",
    "        \n",
    "        #self.proxi11 = Proxi.to(device=self.device)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = torch.abs(x)\n",
    "        x=x.to(device=self.device)\n",
    "        \n",
    "        eye = torch.eye(320)\n",
    "        eye = eye.to(device=self.device)\n",
    "        A = torch.abs(K_space(impulse))\n",
    "        A = A.to(device=self.device)\n",
    "        \n",
    "        \n",
    "        x1 = torch.matmul(A, x)\n",
    "        i = self.proxi1(x1)\n",
    "        x2 = torch.matmul(i, (eye-self.mu1*torch.matmul(torch.transpose(A, 0,1), A)))\n",
    "\n",
    "        x3 = self.mu3 * torch.matmul(A, x)\n",
    "        i = x2 + x3\n",
    "        i = self.proxi3(i)\n",
    "        x4 = torch.matmul(i, (eye-self.mu3*torch.matmul(torch.transpose(A, 0,1), A)))\n",
    "\n",
    "        x5 = self.mu5 * torch.matmul(A, x)\n",
    "        i = x4 + x5\n",
    "        i = self.proxi5(i)\n",
    "        x6 = torch.matmul(i, (eye-self.mu5*torch.matmul(torch.transpose(A, 0,1), A)))\n",
    "\n",
    "        x7 = self.mu7 * torch.matmul(A, x)\n",
    "        i = x6 + x7\n",
    "        i = self.proxi7(i)\n",
    "        #x8 = torch.matmul(i, (eye-self.mu7*torch.matmul(torch.transpose(A, 0,1), A)))\n",
    "        \n",
    "        #x9 = self.mu9 * torch.matmul(A, x)\n",
    "        #i = x8 + x9\n",
    "        #i = self.proxi9(i)\n",
    "        #x10 = i @ (eye-self.mu9*A.T @ A)\n",
    "        \n",
    "        #x11 = self.mu11 * A @ x\n",
    "        #i = x10 + x11\n",
    "        #out = self.proxi11(i)\n",
    "        print(f\"\\n mu3 is {self.mu3} and mu5 is {self.mu5}\")\n",
    "        out = self.proxi7(i)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def test(self,x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 320, 320])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "kspace, M, gt = next(iter(test_loader))\n",
    "m = newISTA(device='cpu')\n",
    "m.eval()\n",
    "this = m.test(acc_img(kspace).unsqueeze(1))\n",
    "this.squeeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on:  cuda\n",
      "\n",
      "Training is about to start on cuda\n",
      "\n",
      "\n",
      "Training epoch 0: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 71.80 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-cfe457850bff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreconstructed_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# backpropagate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-4e67220dba62>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproxi1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmu1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-76369d9022a0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mxabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxabs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;31m#out = nn.functional.log_softmax(img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 443\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 4.00 GiB total capacity; 2.47 GiB already allocated; 71.80 MiB free; 2.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# keep track of losses\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "loss_dict = {'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "\n",
    "# set device\n",
    "dtype = torch.float32\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Running on: ', device)\n",
    "\n",
    "\n",
    "epochs=10\n",
    "lamda = 0.3\n",
    "K = 100\n",
    "model = newISTA(device=device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5e-3, weight_decay=0.0005)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "#mu*torch.matmul(y_flat,(A.T)) + (x_in@A-mu*x_in@A.T@A)\n",
    "\n",
    "print(f\"\\nTraining is about to start on {device}\\n\")\n",
    "for e in range(epochs):\n",
    "    print(f\"\\nTraining epoch {e}: \\t\")\n",
    "    mia_loss = 0\n",
    "    for batch_idx,(kspace, M, gt) in enumerate(tqdm(train_loader)):\n",
    "        # prepare for training\n",
    "        model.train(True)  \n",
    "        optimizer.zero_grad()\n",
    "        # move to gpu if available\n",
    "        #A = A.to(device=device)\n",
    "        reconstructed_img = acc_img(kspace)\n",
    "        #reconstructed_img = reconstructed_img.to(device=device)\n",
    "        #M = M.to(device=device)\n",
    "        gt = gt.to(device=device)\n",
    "        # forward pass\n",
    "        out = model(reconstructed_img.unsqueeze(1))\n",
    "        out = out.squeeze(1)\n",
    "        # backpropagate loss\n",
    "        loss = criterion(out, gt)\n",
    "        loss = loss.to(torch.float32)\n",
    "        loss.backward()\n",
    "        mia_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # update loss dictionary\n",
    "    loss_dict['train_loss'].append(mia_loss/(batch_idx+1))\n",
    "\n",
    "    # print epoch loss\n",
    "    print(f\"\\nEpoch training loss is {mia_loss/(batch_idx+1)}\")\n",
    "\n",
    "    with torch.no_grad():  # setting so that no backprop happens during validation\n",
    "        model.eval()  # setting so that no backprop happens during validation\n",
    "        vloss = 0\n",
    "        for (kspace, M, gt) in tqdm(test_loader):\n",
    "            reconstructed_img = acc_img(kspace)\n",
    "            reconstructed_img=reconstructed_img.to(device=device)\n",
    "            #M = M.to(device=device)\n",
    "            A = A.to(device=device)\n",
    "            gt = gt.to(device=device)\n",
    "            out = model(reconstructed_img.unsqueeze(1))\n",
    "            out = out.squeeze(1)\n",
    "            vloss += criterion(out, gt).to(torch.float16).item()\n",
    "        # update loss dictionary  \n",
    "        loss_dict['val_loss'].append(vloss/len(test_loader))\n",
    "        # print validation loss\n",
    "        print(f\"\\nEpoch validation loss is {vloss/len(test_loader)}\")\n",
    "\n",
    "# save trained model\n",
    "model_path = os.getcwd() + '\\ModelnewISTA.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8000, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.mu3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6582fcdad78c9817c7b8382b86eb25db2029e48a96f1080b267c5349ec26f2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
