{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises Week 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20167271\\Desktop\\ML for signal processing\\5LSL0-GIT\\Assignment 4\n"
     ]
    }
   ],
   "source": [
    "# %% imports\n",
    "# libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# local imports\n",
    "import Fast_MRI_dataloader\n",
    "# import autoencoder_template\n",
    "from matplotlib import pyplot as plt\n",
    "from Fast_MRI_dataloader import create_dataloaders\n",
    "\n",
    "# set-up folders\n",
    "data_loc = os.getcwd()\n",
    "print(data_loc)\n",
    "data_loc = data_loc + '\\Fast_MRI_Knee'\n",
    "batch_size = 16\n",
    "train_loader, val_loader = create_dataloaders(data_loc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - ConvNet\n",
    "As a starting point for a deep learning solution, design an end-to-end convolutional neural\n",
    "network. As input it should take an initial reconstruction from a partial k-space measurement\n",
    "and output a guess for the final reconstruction. The net is not (yet) allowed to make\n",
    "use of knowledge about the sampling mask.\n",
    "\n",
    "(a) [1pt] What loss function would be most suitable to train this network, and what\n",
    "underlying assumption have you made by choosing this loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence is a metric to compare the distribution of pixel values between the input and output images. The MSE loss can be suitable as well but we already know that there is loss of spatial information given the mask and we do not want to penalize shifts or blurring, we want the network to reconstruct the original scan as similarly as possible with the input. Since the network does not know the mask, it should penalize the difference in distributions rather than shift in image. We assume the input distribution is symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [3pt] Design a convolutional neural network of no more than 4 layers. Train it for at\n",
    "least 10 epochs and plot both the training loss and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KReconstruct(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KReconstruct, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # 320x320\n",
    "            nn.Conv2d(2, 4, kernel_size=12, stride=2, padding=0,dilation=1),  # 155x155\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(4, 16, kernel_size=7, stride=1, padding=0, dilation=2), # 143x143\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0, dilation=1), # 139x139\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=23, stride=2, padding=1, dilation=2, output_padding=1) # 320x320\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        xreal = torch.real(x)\n",
    "        ximag = torch.imag(x)\n",
    "        x = torch.cat((xreal, ximag), 1)\n",
    "        \n",
    "        img = self.model(x)\n",
    "        out = nn.functional.log_softmax(img)\n",
    "        return img, out\n",
    "\n",
    "\n",
    "model = KReconstruct()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "#criterion = nn.KLDivLoss()\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on:  cuda\n",
      "\n",
      "Training is about to start on cuda\n",
      "\n",
      "\n",
      "Training epoch 0: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20167271\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 47/47 [00:23<00:00,  1.97it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.4004893201462766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.81it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.25843048095703125\n",
      "\n",
      "Training epoch 1: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:24<00:00,  1.89it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.24529120262632978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.82it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.23444366455078125\n",
      "\n",
      "Training epoch 2: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:24<00:00,  1.95it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.22791825964095744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.83it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.21816253662109375\n",
      "\n",
      "Training epoch 3: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:25<00:00,  1.86it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.2136256441156915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.79it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.20562744140625\n",
      "\n",
      "Training epoch 4: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:24<00:00,  1.93it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.20074073304521275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.81it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.19397735595703125\n",
      "\n",
      "Training epoch 5: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:25<00:00,  1.84it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.1892115816156915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.86it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.1839141845703125\n",
      "\n",
      "Training epoch 6: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:23<00:00,  1.97it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.17880963264627658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.87it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.17330169677734375\n",
      "\n",
      "Training epoch 7: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:26<00:00,  1.81it/s]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch training loss is 0.16933489860372342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:05<00:00,  2.76it/s]\n",
      "  0%|          | 0/47 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch validation loss is 0.16675567626953125\n",
      "\n",
      "Training epoch 8: \t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 17/47 [00:08<00:15,  2.00it/s]"
     ]
    }
   ],
   "source": [
    "# Begin training for 10 epochs\n",
    "\n",
    "epochs=10\n",
    "# set device\n",
    "dtype = torch.float32\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Running on: ', device)\n",
    "model = model.to(device=device)\n",
    "# keep track of losses\n",
    "eval_dic = {'Loss_t': [], 'train_acc': [],'Loss_v': [], 'valid_acc': []}\n",
    "loss_train = []\n",
    "loss_val = []\n",
    "loss_dict = {'train_loss':[], 'val_loss':[]}\n",
    "\n",
    "\n",
    "print(f\"\\nTraining is about to start on {device}\\n\")\n",
    "for e in range(epochs):\n",
    "    print(f\"\\nTraining epoch {e}: \\t\")\n",
    "    mia_loss = 0\n",
    "    for batch_idx,(kspace, M, gt) in enumerate(tqdm(train_loader)):\n",
    "        # prepare for training\n",
    "        model.train(True)  \n",
    "        optimizer.zero_grad()\n",
    "        # move to gpu if available\n",
    "        kspace = kspace.to(device=device)\n",
    "        #M = M.to(device=device)\n",
    "        gt = gt.to(device=device)\n",
    "        # forward pass\n",
    "        out,_ = model(kspace.unsqueeze(1))\n",
    "        out = out.squeeze(1)\n",
    "        # backpropagate loss\n",
    "        loss = criterion(out, gt)\n",
    "        loss = loss.to(torch.float16)\n",
    "        loss.backward()\n",
    "        mia_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # update loss dictionary\n",
    "    loss_dict['train_loss'].append(mia_loss/(batch_idx+1))\n",
    "\n",
    "    # print epoch loss\n",
    "    print(f\"\\nEpoch training loss is {mia_loss/(batch_idx+1)}\")\n",
    "\n",
    "    with torch.no_grad():  # setting so that no backprop happens during validation\n",
    "        model.eval()  # setting so that no backprop happens during validation\n",
    "        vloss = 0\n",
    "        for (kspace, M, gt) in tqdm(val_loader):\n",
    "            kspace = kspace.to(device=device)\n",
    "            #M = M.to(device=device)\n",
    "            gt = gt.to(device=device)\n",
    "            out,_ = model(kspace.unsqueeze(1))\n",
    "            out = out.squeeze(1)\n",
    "            vloss += criterion(out, gt).to(torch.float16).item()\n",
    "        # update loss dictionary  \n",
    "        loss_dict['val_loss'].append(vloss)\n",
    "        # print validation loss\n",
    "        print(f\"\\nEpoch validation loss is {vloss/len(val_loader)}\")\n",
    "\n",
    "# save trained model\n",
    "model_path = os.getcwd() + '\\Modelex5.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-0ed338e96e7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m plt.plot(range(epochs), loss_dic['train_loss'], \n\u001b[1;32m----> 6\u001b[1;33m          label='training loss', linewidth=5)\n\u001b[0m\u001b[0;32m      7\u001b[0m plt.plot(range(epochs), eval_dic['val_loss'], \n\u001b[0;32m      8\u001b[0m          label='test loss', linestyle='--', linewidth=5)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2840\u001b[0m     return gca().plot(\n\u001b[0;32m   2841\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2842\u001b[1;33m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[0;32m   2843\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1741\u001b[0m         \"\"\"\n\u001b[0;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "\n",
    "#validation loss is actually test loss\n",
    "plt.figure(figsize=(15,10),dpi=100)\n",
    "plt.plot(range(epochs), loss_dic['train_loss'], \n",
    "         label='training loss', linewidth=5)\n",
    "plt.plot(range(epochs), eval_dic['val_loss'], \n",
    "         label='test loss', linestyle='--', linewidth=5)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.grid()\n",
    "plt.legend(loc=0, prop={'size': 20})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6582fcdad78c9817c7b8382b86eb25db2029e48a96f1080b267c5349ec26f2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
